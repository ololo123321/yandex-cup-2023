defaults:
  - _self_
  - hydra: default

training_args:
  _target_: transformers.TrainingArguments

  do_train: true
  do_eval: true

  output_dir: ???

  # training steps
  num_train_epochs: 3
  max_steps: -1  # default -1; overrides "num_train_epochs"

  # evaluation
  evaluation_strategy: epoch  # steps
  eval_steps: 5000

  # batching
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1

  # lr
  learning_rate: 1e-4
  lr_scheduler_type: linear
  warmup_ratio: 0.1

  # regularization
  weight_decay: 0.01
  max_grad_norm: 1.0

  # saving
  save_strategy: ${training_args.evaluation_strategy}
  save_steps: ${training_args.eval_steps}
  save_total_limit: 3
  save_on_each_node: false
  load_best_model_at_end: true  # must be true with EarlyStoppingCallback

  # logging
  logging_strategy: steps
  logging_steps: 500

  # fp16
  fp16: true
  half_precision_backend: auto  # or cuda_amp
  fp16_opt_level: O1  # for apex
  fp16_full_eval: false

  # data pipeline
  dataloader_num_workers: 0
  dataloader_pin_memory: true

  # other
  no_cuda: false
  seed: 42
  disable_tqdm: false
  metric_for_best_model: map
  greater_is_better: true
  sharded_ddp: simple  # need fairscale (pip install fairscale)
  skip_memory_metrics: true
  label_smoothing_factor: 0.0
  ddp_find_unused_parameters: false

model:
  _target_: ???

training_dataset:
  _target_: src.datasets.TrainingDataset
  data: null
  p_catmix: 0.0
  p_cutmix: 0.0
  cache_emb: false

valid_dataset:
  _target_: src.datasets.TrainingDataset
  data: null

collator:
  _target_: src.collators.TrainingCollator
  num_classes: 256
  max_length: 150  # TODO: вынести в dataset

trainer_cls:
  _target_: hydra.utils.get_class
  path: src.trainers.TrainerV1

trainer_params:
  save_weights_only: true
  pos_weight_path: null

# other
train_data_path: ???
valid_data_path: ???
embeddings_dir: ???
num_train_examples: null
num_valid_examples: null
checkpoint_path: null
model_dir: null